{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtZKFPclx70pfyucoGVc7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gz2214/Text2Trade/blob/main/code/Bert_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsDRkTJgUPQA",
        "outputId": "d0b38563-1f96-4928-bdc2-c0e97e6d6849"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hxtJP0iuR_i4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_from_disk\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import ast\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OP4ReU6JTXwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7844502a-dbf4-4eab-cd93-a906f60def27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path_block0 = '/content/drive/MyDrive/Text2Trade/data/block0'\n",
        "dataset_path_block1 = '/content/drive/MyDrive/Text2Trade/data/block1'\n",
        "dataset_path_block2 = '/content/drive/MyDrive/Text2Trade/data/block2'\n",
        "dataset_path_block3 = '/content/drive/MyDrive/Text2Trade/data/block3'\n",
        "dataset_path_block4 = '/content/drive/MyDrive/Text2Trade/data/block4'\n",
        "\n",
        "block0 = load_from_disk(dataset_path_block0)\n",
        "block1 = load_from_disk(dataset_path_block1)\n",
        "block2 = load_from_disk(dataset_path_block2)\n",
        "block3 = load_from_disk(dataset_path_block3)\n",
        "block4 = load_from_disk(dataset_path_block4)\n",
        "\n",
        "blocks = [block0, block1, block2, block3, block4]\n",
        "block_names = ['block0', 'block1', 'block2', 'block3', 'block4']"
      ],
      "metadata": {
        "id": "SrC41F-nSBzV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=2 # Binary classification (up/down)\n",
        ")"
      ],
      "metadata": {
        "id": "9EK4tpIUSGkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2369fb26-1fba-49f2-9f1c-517397277391"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to split the text into chunks of tokens - ensure each chunk is less than 512 tokens\n",
        "def chunk_tokens(text, max_tokens=512):\n",
        "    tokens = tokenizer.tokenize(text) #tokenize text\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        current_chunk.append(token) #append token to current chunk\n",
        "        current_length += 1\n",
        "\n",
        "        #once the limit is reached, the current chunk is saved, and a new chunk begins\n",
        "        if current_length >= max_tokens:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "euRYSa1KT6Y2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process a block of data and create df_train and df_test\n",
        "def process_blocks(block, block_name):\n",
        "    train_dataset = block['train']\n",
        "    test_dataset = block['test']\n",
        "\n",
        "    train_dates, train_titles, train_labels = [], [], []\n",
        "    test_dates, test_titles, test_labels = [], [], []\n",
        "\n",
        "    for item in tqdm(train_dataset, desc=f'Processing Train Data ({block_name})'):\n",
        "        date = item['Date']\n",
        "        title = item['Title']\n",
        "        label = item['Label']\n",
        "        #tokenizes the titles into chunks\n",
        "        chunks = chunk_tokens(title)\n",
        "\n",
        "        for chunk in chunks:\n",
        "            train_dates.append(date)\n",
        "            train_titles.append(' '.join(chunk))\n",
        "            train_labels.append(label)\n",
        "\n",
        "    for item in tqdm(test_dataset, desc=f'Processing Test Data ({block_name})'):\n",
        "        date = item['Date']\n",
        "        title = item['Title']\n",
        "        label = item['Label']\n",
        "        chunks = chunk_tokens(title)\n",
        "\n",
        "        for chunk in chunks:\n",
        "            test_dates.append(date)\n",
        "            test_titles.append(' '.join(chunk))\n",
        "            test_labels.append(label)\n",
        "\n",
        "    df_train = pd.DataFrame({'Date': train_dates, 'Title': train_titles, 'Label': train_labels})\n",
        "    df_test = pd.DataFrame({'Date': test_dates, 'Title': test_titles, 'Label': test_labels})\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "# Process each block\n",
        "for block, block_name in zip(blocks, block_names):\n",
        "    df_train, df_test = process_blocks(block, block_name)\n",
        "    df_train.to_csv(f'/content/drive/MyDrive/Text2Trade/data/{block_name}_train.csv', index=False)\n",
        "    df_test.to_csv(f'/content/drive/MyDrive/Text2Trade/data/{block_name}_test.csv', index=False)"
      ],
      "metadata": {
        "id": "RaOeaixUT_CY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41e52de-3b74-4ed7-8713-26f0d3e6cd5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Train Data (block0): 100%|██████████| 128/128 [00:04<00:00, 29.50it/s]\n",
            "Processing Test Data (block0): 100%|██████████| 32/32 [00:01<00:00, 28.93it/s]\n",
            "Processing Train Data (block1): 100%|██████████| 128/128 [00:04<00:00, 26.94it/s]\n",
            "Processing Test Data (block1): 100%|██████████| 32/32 [00:01<00:00, 29.42it/s]\n",
            "Processing Train Data (block2): 100%|██████████| 127/127 [00:04<00:00, 26.76it/s]\n",
            "Processing Test Data (block2): 100%|██████████| 32/32 [00:01<00:00, 28.82it/s]\n",
            "Processing Train Data (block3): 100%|██████████| 127/127 [00:04<00:00, 30.51it/s]\n",
            "Processing Test Data (block3): 100%|██████████| 32/32 [00:01<00:00, 30.55it/s]\n",
            "Processing Train Data (block4): 100%|██████████| 127/127 [00:04<00:00, 30.25it/s]\n",
            "Processing Test Data (block4): 100%|██████████| 32/32 [00:01<00:00, 31.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = block0['train']\n",
        "num_examples_to_view = 5\n",
        "example_text = train_dataset.select(range(num_examples_to_view))['Title']"
      ],
      "metadata": {
        "id": "DdpdtbyeUH4i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model\n",
        "\n",
        "Train Bert Classifier using data from block 0 - block 3. No validation required in this step."
      ],
      "metadata": {
        "id": "nHH-au8DdJmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def load_train_data(blocks, tokenizer, max_tokens=512, batch_size=16, shuffle=True, train_ratio=0.8):\n",
        "    # Create empty lists to store data from blocks\n",
        "    titles = []\n",
        "    labels = []\n",
        "\n",
        "    # Merge data from blocks 0 to 3\n",
        "    for block in blocks[:4]:  # Blocks 0 to 3\n",
        "        # Extract titles and labels\n",
        "        block_titles = block['train']['Title']\n",
        "        block_labels = block['train']['Label']\n",
        "\n",
        "        # Append titles and labels\n",
        "        titles.extend(block_titles)\n",
        "        labels.extend(block_labels)\n",
        "\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Tokenize and process titles\n",
        "    input_ids_list = []\n",
        "    attention_masks_list = []\n",
        "\n",
        "    for title in titles:\n",
        "        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_masks_list.append(attention_mask)\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\n",
        "\n",
        "    # Pad input_ids and attention_mask tensors to have the same length\n",
        "    for i in range(len(input_ids_list)):\n",
        "        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "\n",
        "    # Stack input_ids tensors and attention_mask tensors along dim=0\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_masks = torch.stack(attention_masks_list)\n",
        "\n",
        "    # Split data into train and validation sets based on train_ratio\n",
        "    train_size = int(train_ratio * len(labels))\n",
        "    val_size = len(labels) - train_size\n",
        "\n",
        "    train_dataset = TensorDataset(input_ids[:train_size], attention_masks[:train_size], labels[:train_size])\n",
        "    val_dataset = TensorDataset(input_ids[train_size:], attention_masks[train_size:], labels[train_size:])\n",
        "\n",
        "    # Create DataLoader for train and validation sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "train_dataloader, val_dataloader = load_train_data(blocks, tokenizer, batch_size=16, shuffle=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IPmVbeRiVMDu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "c685092d-52b6-4bab-a85c-85d4291ddaba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef load_train_data(blocks, tokenizer, max_tokens=512, batch_size=16, shuffle=True, train_ratio=0.8):\\n    # Create empty lists to store data from blocks\\n    titles = []\\n    labels = []\\n\\n    # Merge data from blocks 0 to 3\\n    for block in blocks[:4]:  # Blocks 0 to 3\\n        # Extract titles and labels\\n        block_titles = block['train']['Title']\\n        block_labels = block['train']['Label']\\n\\n        # Append titles and labels\\n        titles.extend(block_titles)\\n        labels.extend(block_labels)\\n\\n    # Convert labels to a tensor\\n    labels = torch.tensor(labels, dtype=torch.long)\\n\\n    # Tokenize and process titles\\n    input_ids_list = []\\n    attention_masks_list = []\\n\\n    for title in titles:\\n        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\\n        input_ids = inputs['input_ids']\\n        attention_mask = inputs['attention_mask']\\n        input_ids_list.append(input_ids)\\n        attention_masks_list.append(attention_mask)\\n\\n    # Find the maximum sequence length\\n    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\\n\\n    # Pad input_ids and attention_mask tensors to have the same length\\n    for i in range(len(input_ids_list)):\\n        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\\n        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\\n\\n    # Stack input_ids tensors and attention_mask tensors along dim=0\\n    input_ids = torch.stack(input_ids_list)\\n    attention_masks = torch.stack(attention_masks_list)\\n\\n    # Split data into train and validation sets based on train_ratio\\n    train_size = int(train_ratio * len(labels))\\n    val_size = len(labels) - train_size\\n\\n    train_dataset = TensorDataset(input_ids[:train_size], attention_masks[:train_size], labels[:train_size])\\n    val_dataset = TensorDataset(input_ids[train_size:], attention_masks[train_size:], labels[train_size:])\\n\\n    # Create DataLoader for train and validation sets\\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\\n    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\n\\n    return train_dataloader, val_dataloader\\n\\ntrain_dataloader, val_dataloader = load_train_data(blocks, tokenizer, batch_size=16, shuffle=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "block0 - block4['train']"
      ],
      "metadata": {
        "id": "l6rBSKLA_bfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_data(blocks, tokenizer, max_tokens=512, batch_size=16, shuffle=True, train_ratio=0.8):\n",
        "    # Create empty lists to store data from blocks\n",
        "    titles = []\n",
        "    labels = []\n",
        "\n",
        "    # Merge data from blocks 0 to 3\n",
        "    for block in blocks[:4]:  # Blocks 0 to 3\n",
        "        # Extract titles and labels\n",
        "        block_titles = block['train']['Title']\n",
        "        block_labels = block['train']['Label']\n",
        "\n",
        "        # Append titles and labels\n",
        "        titles.extend(block_titles)\n",
        "        labels.extend(block_labels)\n",
        "\n",
        "    # Merge data from block4['train']\n",
        "    block4_train_titles = blocks[4]['train']['Title']\n",
        "    block4_train_labels = blocks[4]['train']['Label']\n",
        "\n",
        "    # Append titles and labels\n",
        "    titles.extend(block4_train_titles)\n",
        "    labels.extend(block4_train_labels)\n",
        "\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Tokenize and process titles\n",
        "    input_ids_list = []\n",
        "    attention_masks_list = []\n",
        "\n",
        "    for title in titles:\n",
        "        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_masks_list.append(attention_mask)\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\n",
        "\n",
        "    # Pad input_ids and attention_mask tensors to have the same length\n",
        "    for i in range(len(input_ids_list)):\n",
        "        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "\n",
        "    # Stack input_ids tensors and attention_mask tensors along dim=0\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_masks = torch.stack(attention_masks_list)\n",
        "\n",
        "    # Split data into train and validation sets based on train_ratio\n",
        "    train_size = int(train_ratio * len(labels))\n",
        "    val_size = len(labels) - train_size\n",
        "\n",
        "    train_dataset = TensorDataset(input_ids[:train_size], attention_masks[:train_size], labels[:train_size])\n",
        "    val_dataset = TensorDataset(input_ids[train_size:], attention_masks[train_size:], labels[train_size:])\n",
        "\n",
        "    # Create DataLoader for train and validation sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "train_dataloader, val_dataloader = load_train_data(blocks, tokenizer, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "4E3gJJwO_bA7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataloader to view the first batch\n",
        "for batch in train_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    print(\"Input IDs:\", input_ids)\n",
        "    print(\"Attention Mask:\", attention_mask)\n",
        "    print(\"Labels:\", labels)\n",
        "    break  # Stop after viewing the first batch"
      ],
      "metadata": {
        "id": "IVTX5lf3d0uC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc259608-7708-40a3-a093-74842ff6a9b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[[  101,  1047,  2527,  ...,  3796,  1048,   102]],\n",
            "\n",
            "        [[  101,  5899,  2003,  ...,  1998,  2062,   102]],\n",
            "\n",
            "        [[  101, 29433,  1005,  ...,  1521,  1055,   102]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[  101, 15768,  4139,  ...,  2006,  1029,   102]],\n",
            "\n",
            "        [[  101, 28517,  2063,  ...,  2539, 28896,   102]],\n",
            "\n",
            "        [[  101, 26060, 17394,  ..., 17404,  4897,   102]]])\n",
            "Attention Mask: tensor([[[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1]]])\n",
            "Labels: tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer with weight decay\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
        "\n",
        "#we want to leave the last block as \"test\" after fine tuning.\n",
        "epochs = 60\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "H1Dl63ATeFw6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  # ========================================\n",
        "  #               Training\n",
        "  # ========================================\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{epochs}')\n",
        "  print('Training...')\n",
        "\n",
        "  avg_train_loss=[]\n",
        "  total_train_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    b_input_ids = batch[0].squeeze(1)\n",
        "    b_input_mask = batch[1]\n",
        "    b_labels = batch[2]\n",
        "\n",
        "    #print(\"Shape of input_ids:\", b_input_ids.shape)\n",
        "\n",
        "    model.zero_grad()\n",
        "    outputs = model(input_ids = b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "    #loss = outputs.loss\n",
        "    loss = nn.CrossEntropyLoss()(outputs.logits, b_labels)\n",
        "    logits = outputs.logits\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "  print(\"Average training loss: {0:.2f}\".format(avg_train_loss))"
      ],
      "metadata": {
        "id": "1KZzR6JgeMIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253e68fe-3126-41ff-d821-5fa462d11554"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "Training...\n",
            "Average training loss: 0.70\n",
            "Epoch 2/60\n",
            "Training...\n",
            "Average training loss: 0.69\n",
            "Epoch 3/60\n",
            "Training...\n",
            "Average training loss: 0.68\n",
            "Epoch 4/60\n",
            "Training...\n",
            "Average training loss: 0.65\n",
            "Epoch 5/60\n",
            "Training...\n",
            "Average training loss: 0.58\n",
            "Epoch 6/60\n",
            "Training...\n",
            "Average training loss: 0.50\n",
            "Epoch 7/60\n",
            "Training...\n",
            "Average training loss: 0.35\n",
            "Epoch 8/60\n",
            "Training...\n",
            "Average training loss: 0.24\n",
            "Epoch 9/60\n",
            "Training...\n",
            "Average training loss: 0.16\n",
            "Epoch 10/60\n",
            "Training...\n",
            "Average training loss: 0.14\n",
            "Epoch 11/60\n",
            "Training...\n",
            "Average training loss: 0.11\n",
            "Epoch 12/60\n",
            "Training...\n",
            "Average training loss: 0.07\n",
            "Epoch 13/60\n",
            "Training...\n",
            "Average training loss: 0.03\n",
            "Epoch 14/60\n",
            "Training...\n",
            "Average training loss: 0.03\n",
            "Epoch 15/60\n",
            "Training...\n",
            "Average training loss: 0.03\n",
            "Epoch 16/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 17/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 18/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 19/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 20/60\n",
            "Training...\n",
            "Average training loss: 0.03\n",
            "Epoch 21/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 22/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 23/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 24/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 25/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 26/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 27/60\n",
            "Training...\n",
            "Average training loss: 0.02\n",
            "Epoch 28/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 29/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 30/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 31/60\n",
            "Training...\n",
            "Average training loss: 0.02\n",
            "Epoch 32/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 33/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 34/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 35/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 36/60\n",
            "Training...\n",
            "Average training loss: 0.02\n",
            "Epoch 37/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 38/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 39/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 40/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 41/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 42/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 43/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 44/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 45/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 46/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 47/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 48/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 49/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 50/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 51/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 52/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 53/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 54/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 55/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 56/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 57/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 58/60\n",
            "Training...\n",
            "Average training loss: 0.01\n",
            "Epoch 59/60\n",
            "Training...\n",
            "Average training loss: 0.00\n",
            "Epoch 60/60\n",
            "Training...\n",
            "Average training loss: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "gU3b4tWreNhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_for_testing(blocks, tokenizer, max_tokens=512, batch_size=16, shuffle=True):\n",
        "    # Create empty lists to store data\n",
        "    titles = []\n",
        "    labels = []\n",
        "    input_ids_list = []  # Add this line\n",
        "    attention_masks_list = []  # Add this line\n",
        "\n",
        "    # Merge data from blocks 0 to 4 for training\n",
        "    for block in blocks[:5]:  # Blocks 0 to 4\n",
        "        # Extract titles and labels\n",
        "        block_titles = block['train']['Title']\n",
        "        block_labels = block['train']['Label']\n",
        "\n",
        "        # Append titles and labels\n",
        "        titles.extend(block_titles)\n",
        "        labels.extend(block_labels)\n",
        "\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Tokenize and process titles\n",
        "    for title in titles:\n",
        "        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_masks_list.append(attention_mask)\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\n",
        "\n",
        "    # Pad input_ids and attention_mask tensors to have the same length\n",
        "    for i in range(len(input_ids_list)):\n",
        "        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "\n",
        "    # Stack input_ids tensors and attention_mask tensors along dim=0\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_masks = torch.stack(attention_masks_list)\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    # Extract day-level labels and titles for testing\n",
        "    test_dates = blocks[4]['test']['Date']\n",
        "    test_titles = blocks[4]['test']['Title']\n",
        "    test_labels = blocks[4]['test']['Label']\n",
        "\n",
        "    # Tokenize and process titles for testing\n",
        "    test_input_ids_list = []\n",
        "    test_attention_masks_list = []\n",
        "\n",
        "    for title in test_titles:\n",
        "        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        test_input_ids_list.append(input_ids)\n",
        "        test_attention_masks_list.append(attention_mask)\n",
        "\n",
        "    # Find the maximum sequence length for testing data\n",
        "    max_test_sequence_length = max(input_ids.shape[1] for input_ids in test_input_ids_list)\n",
        "\n",
        "    # Pad input_ids and attention_mask tensors for testing data to have the same length\n",
        "    for i in range(len(test_input_ids_list)):\n",
        "        test_input_ids_list[i] = torch.cat([test_input_ids_list[i], torch.zeros(1, max_test_sequence_length - test_input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "        test_attention_masks_list[i] = torch.cat([test_attention_masks_list[i], torch.zeros(1, max_test_sequence_length - test_attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "\n",
        "    # Stack input_ids tensors and attention_mask tensors for testing data along dim=0\n",
        "    test_input_ids = torch.stack(test_input_ids_list)\n",
        "    test_attention_masks = torch.stack(test_attention_masks_list)\n",
        "\n",
        "    # Convert labels for testing data to a tensor\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader for testing data\n",
        "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "train_dataloader, test_dataloader = load_data_for_testing(blocks, tokenizer, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "Gl49ZqWSs3VB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches_in_test_dataloader = len(test_dataloader)\n",
        "print(\"Number of batches in test_dataloader:\", num_batches_in_test_dataloader)\n",
        "\n",
        "batch_size = 16\n",
        "total_examples_in_test_dataloader = num_batches_in_test_dataloader * batch_size\n",
        "print(\"Total number of examples in test_dataloader:\", total_examples_in_test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOv4nFzGDP1X",
        "outputId": "dc75a423-613a-41ad-c9e8-1962b241dc3b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in test_dataloader: 2\n",
            "Total number of examples in test_dataloader: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, average_precision_score"
      ],
      "metadata": {
        "id": "EEmDfXdiwifo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each day has different number of chunks"
      ],
      "metadata": {
        "id": "Sbu81wOZ8DEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_num_chunks_per_day(dataset):\n",
        "    # Convert the dataset to a DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Group by unique \"Date\" and count the number of chunks (labels) per day\n",
        "    num_chunks_per_day = df.groupby(\"Date\")[\"Label\"].count().values\n",
        "\n",
        "    return num_chunks_per_day\n",
        "\n",
        "block4_test = pd.read_csv(\"/content/drive/MyDrive/Text2Trade/data/block4_test.csv\")  # Replace \"path_to_block4_test.csv\" with the actual file path\n",
        "num_chunks_per_day = calculate_num_chunks_per_day(block4_test)\n",
        "num_chunks_per_day"
      ],
      "metadata": {
        "id": "LLMG3Esn7EZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506cef1e-f787-4f0e-c339-d3e60f37bf95"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  3,  9,  3,  3,  3,  3,  9,  3,  3,  3,  3,  9,  3,  4,  3,  4,\n",
              "        9,  3,  3,  4,  3,  9,  4,  3,  3,  3, 10,  3,  3,  3,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "day_predictions = [] #store predictions (logits) for each chunk of text\n",
        "day_probabilities = [] #store probability of class 1 for each chunk\n",
        "day_ground_truth_labels = [] #store ground truth labels for each chunk\n",
        "num_examples_per_day = None\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        #extract inputs for a day (chunks of text)\n",
        "        day_input_ids = batch[0].squeeze(1)\n",
        "        day_attention_mask = batch[1]\n",
        "        day_labels = batch[2]\n",
        "\n",
        "        outputs = model(input_ids=day_input_ids, attention_mask=day_attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        #aggregate predictions for the day\n",
        "        day_predictions.append(torch.argmax(logits, dim=1))\n",
        "        day_probabilities.append(probabilities[:, 1])  # Probability of class 1\n",
        "\n",
        "        # Ground truth labels for the day\n",
        "        day_ground_truth_labels.append(day_labels)\n",
        "\n",
        "day_predictions = torch.cat(day_predictions)\n",
        "print(\"Shape of day_predictions:\", day_predictions.shape)\n",
        "day_probabilities = torch.cat(day_probabilities)\n",
        "day_ground_truth_labels = torch.cat(day_ground_truth_labels)\n",
        "\n",
        "#majority vote for each day\n",
        "day_final_labels = []\n",
        "current_index = 0\n",
        "\n",
        "for num_chunks in num_chunks_per_day:\n",
        "    day_predictions_i = day_predictions[current_index:current_index + num_chunks]\n",
        "    day_final_label = 1 if (day_predictions_i.sum() > (num_chunks / 2)) else 0\n",
        "    day_final_labels.extend([day_final_label] * num_chunks)\n",
        "    current_index += num_chunks\n",
        "\n",
        "#probability of class 1 for each day\n",
        "day_probabilities = [day_probabilities[i:i + num_chunks] for i, num_chunks in enumerate(num_chunks_per_day)]\n",
        "day_avg_probabilities = [torch.mean(prob) for prob in day_probabilities]\n",
        "\n",
        "#ensure that day_final_labels and day_ground_truth_labels have the same length\n",
        "day_final_labels = day_final_labels[:len(day_ground_truth_labels)]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(day_ground_truth_labels, day_final_labels)\n",
        "f1 = f1_score(day_ground_truth_labels, day_final_labels)\n",
        "precision = precision_score(day_ground_truth_labels, day_final_labels)\n",
        "map = average_precision_score(day_ground_truth_labels, day_final_labels)\n",
        "\n",
        "#print(\"Length of day_ground_truth_labels:\", len(day_ground_truth_labels))\n",
        "#print(\"Length of day_final_labels:\", len(day_final_labels))\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Mean Average Precision: {map:.2f}\")"
      ],
      "metadata": {
        "id": "WPCRsVw78oiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b457c5-9519-4dd6-e124-6757299bb07c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of day_predictions: torch.Size([32])\n",
            "Accuracy: 0.47\n",
            "F1 Score: 0.54\n",
            "Precision: 0.48\n",
            "Mean Average Precision: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save output into a text file"
      ],
      "metadata": {
        "id": "A6k_Tp86Henf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict = {\n",
        "    \"Precision\": precision,\n",
        "    \"Mean Average Precision\": map,\n",
        "    \"F1 Score\": f1,\n",
        "    \"Accuracy\": accuracy\n",
        "}\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/Text2Trade/results/BERT_classifier_eval.txt\"\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    for metric_name, metric_value in metrics_dict.items():\n",
        "        file.write(f\"{metric_name}: {metric_value:.6f}\\n\")\n",
        "\n",
        "print(f\"Metrics saved to {file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdYDpnvrG2O2",
        "outputId": "0cfe16fe-b9b9-4a11-b771-bd5b11672013"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to /content/drive/MyDrive/Text2Trade/results/BERT_classifier_eval.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day-level aggregated outputs (from majority vote):\n"
      ],
      "metadata": {
        "id": "f75rj011AB9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(day_final_labels):\n",
        "    print(f\"Day {i + 1} Aggregated Label: {label}\")"
      ],
      "metadata": {
        "id": "HsEc1A95_2H3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8e6e3b-7393-4931-b4bc-fcddc374cb9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 1 Aggregated Label: 0\n",
            "Day 2 Aggregated Label: 0\n",
            "Day 3 Aggregated Label: 0\n",
            "Day 4 Aggregated Label: 0\n",
            "Day 5 Aggregated Label: 0\n",
            "Day 6 Aggregated Label: 0\n",
            "Day 7 Aggregated Label: 1\n",
            "Day 8 Aggregated Label: 1\n",
            "Day 9 Aggregated Label: 1\n",
            "Day 10 Aggregated Label: 1\n",
            "Day 11 Aggregated Label: 1\n",
            "Day 12 Aggregated Label: 1\n",
            "Day 13 Aggregated Label: 1\n",
            "Day 14 Aggregated Label: 1\n",
            "Day 15 Aggregated Label: 1\n",
            "Day 16 Aggregated Label: 1\n",
            "Day 17 Aggregated Label: 1\n",
            "Day 18 Aggregated Label: 1\n",
            "Day 19 Aggregated Label: 1\n",
            "Day 20 Aggregated Label: 1\n",
            "Day 21 Aggregated Label: 1\n",
            "Day 22 Aggregated Label: 1\n",
            "Day 23 Aggregated Label: 1\n",
            "Day 24 Aggregated Label: 1\n",
            "Day 25 Aggregated Label: 1\n",
            "Day 26 Aggregated Label: 1\n",
            "Day 27 Aggregated Label: 1\n",
            "Day 28 Aggregated Label: 0\n",
            "Day 29 Aggregated Label: 0\n",
            "Day 30 Aggregated Label: 0\n",
            "Day 31 Aggregated Label: 0\n",
            "Day 32 Aggregated Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_name = \"/content/drive/MyDrive/Text2Trade/results/BERT_classifier_daylevel_class.txt\"\n",
        "\n",
        "# Open the file for writing\n",
        "with open(output_file_name, 'w') as output_file:\n",
        "    for i, label in enumerate(day_final_labels):\n",
        "        output_file.write(f\"Day {i + 1} Aggregated Label: {label}\\n\")\n",
        "\n",
        "# Print a message to confirm that the file has been saved\n",
        "print(f\"Day final labels saved to {output_file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XelDn0dGHz3X",
        "outputId": "b56d50d2-3f98-42f8-975e-d84a09dcbd58"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day final labels saved to /content/drive/MyDrive/Text2Trade/results/BERT_classifier_daylevel_class.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Day-level averaged proabaility (for being 1):"
      ],
      "metadata": {
        "id": "kXVduvFDALsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, prob in enumerate(day_avg_probabilities):\n",
        "    print(f\"Day {i + 1} Average Probability of 1: {prob:.2f}\")"
      ],
      "metadata": {
        "id": "TsUUlQvvAIcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13769b2-9d22-48db-9fdd-b41555305248"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 1 Average Probability of 1: 0.33\n",
            "Day 2 Average Probability of 1: 0.67\n",
            "Day 3 Average Probability of 1: 0.40\n",
            "Day 4 Average Probability of 1: 0.33\n",
            "Day 5 Average Probability of 1: 0.00\n",
            "Day 6 Average Probability of 1: 0.21\n",
            "Day 7 Average Probability of 1: 0.54\n",
            "Day 8 Average Probability of 1: 0.57\n",
            "Day 9 Average Probability of 1: 0.67\n",
            "Day 10 Average Probability of 1: 0.63\n",
            "Day 11 Average Probability of 1: 0.30\n",
            "Day 12 Average Probability of 1: 0.50\n",
            "Day 13 Average Probability of 1: 0.62\n",
            "Day 14 Average Probability of 1: 0.53\n",
            "Day 15 Average Probability of 1: 0.75\n",
            "Day 16 Average Probability of 1: 1.00\n",
            "Day 17 Average Probability of 1: 0.75\n",
            "Day 18 Average Probability of 1: 0.76\n",
            "Day 19 Average Probability of 1: 0.67\n",
            "Day 20 Average Probability of 1: 1.00\n",
            "Day 21 Average Probability of 1: 0.72\n",
            "Day 22 Average Probability of 1: 0.62\n",
            "Day 23 Average Probability of 1: 0.54\n",
            "Day 24 Average Probability of 1: 0.72\n",
            "Day 25 Average Probability of 1: 0.67\n",
            "Day 26 Average Probability of 1: 0.33\n",
            "Day 27 Average Probability of 1: 0.33\n",
            "Day 28 Average Probability of 1: 0.60\n",
            "Day 29 Average Probability of 1: 0.67\n",
            "Day 30 Average Probability of 1: 0.67\n",
            "Day 31 Average Probability of 1: 1.00\n",
            "Day 32 Average Probability of 1: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file name for the average probabilities\n",
        "average_probabilities_file_name = \"/content/drive/MyDrive/Text2Trade/results/BERT_classifier_daylevel_prob.txt\"\n",
        "\n",
        "# Open the file for writing\n",
        "with open(average_probabilities_file_name, 'w') as file:\n",
        "    # Write each day's average probability in the desired format\n",
        "    for i, prob in enumerate(day_avg_probabilities):\n",
        "        file.write(f\"Day {i + 1} Average Probability of 1: {prob:.2f}\\n\")\n",
        "\n",
        "# Print a message to confirm that the file has been saved\n",
        "print(f\"Average probabilities saved to {average_probabilities_file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBt9mVGDIFZP",
        "outputId": "cacd3262-d169-4865-9057-53613635ac36"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average probabilities saved to /content/drive/MyDrive/Text2Trade/results/BERT_classifier_daylevel_prob.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT IN USE"
      ],
      "metadata": {
        "id": "i4chJALr-bLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def load_data(data_path, tokenizer, max_tokens=512, batch_size=16, shuffle=True):\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Extract titles, labels, and convert them to lists\n",
        "    titles = df['Title'].tolist()\n",
        "    labels = df['Label'].tolist()\n",
        "\n",
        "    input_ids_list = []\n",
        "    attention_masks_list = []\n",
        "\n",
        "    for title in titles:\n",
        "        #encoding\n",
        "        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\n",
        "\n",
        "        # Extract input_ids tensor and attention_mask tensor from the BatchEncoding\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_masks_list.append(attention_mask)\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\n",
        "\n",
        "    # Pad input_ids and attention_mask tensors to have the same length\n",
        "    for i in range(len(input_ids_list)):\n",
        "        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\n",
        "\n",
        "    # Stack input_ids tensors and attention_mask tensors along dim=0\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_masks = torch.stack(attention_masks_list)\n",
        "\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Create a TensorDataset\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    # Create a DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J-_INSR-Uzl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "a614c8e6-2483-4163-ba6c-c452c6b84e78"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef load_data(data_path, tokenizer, max_tokens=512, batch_size=16, shuffle=True):\\n    df = pd.read_csv(data_path)\\n\\n    # Extract titles, labels, and convert them to lists\\n    titles = df['Title'].tolist()\\n    labels = df['Label'].tolist()\\n\\n    input_ids_list = []\\n    attention_masks_list = []\\n\\n    for title in titles:\\n        #encoding\\n        inputs = tokenizer(title, return_tensors='pt', padding=True, truncation=True, max_length=max_tokens)\\n\\n        # Extract input_ids tensor and attention_mask tensor from the BatchEncoding\\n        input_ids = inputs['input_ids']\\n        attention_mask = inputs['attention_mask']\\n\\n        input_ids_list.append(input_ids)\\n        attention_masks_list.append(attention_mask)\\n\\n    # Find the maximum sequence length\\n    max_sequence_length = max(input_ids.shape[1] for input_ids in input_ids_list)\\n\\n    # Pad input_ids and attention_mask tensors to have the same length\\n    for i in range(len(input_ids_list)):\\n        input_ids_list[i] = torch.cat([input_ids_list[i], torch.zeros(1, max_sequence_length - input_ids_list[i].shape[1], dtype=torch.long)], dim=1)\\n        attention_masks_list[i] = torch.cat([attention_masks_list[i], torch.zeros(1, max_sequence_length - attention_masks_list[i].shape[1], dtype=torch.long)], dim=1)\\n\\n    # Stack input_ids tensors and attention_mask tensors along dim=0\\n    input_ids = torch.stack(input_ids_list)\\n    attention_masks = torch.stack(attention_masks_list)\\n\\n    # Convert labels to a tensor\\n    labels = torch.tensor(labels, dtype=torch.long)\\n\\n    # Create a TensorDataset\\n    dataset = TensorDataset(input_ids, attention_masks, labels)\\n\\n    # Create a DataLoader\\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\\n\\n    return dataloader\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "num_blocks = 4\n",
        "\n",
        "# Lists to store train and test DataLoaders for each block\n",
        "train_dataloaders = []\n",
        "test_dataloaders = []\n",
        "\n",
        "# Load data for each block\n",
        "for i in range(num_blocks):\n",
        "    train_data_path = f'/content/drive/MyDrive/Text2Trade/data/block{i}_train.csv'\n",
        "    test_data_path = f'/content/drive/MyDrive/Text2Trade/data/block{i}_test.csv'\n",
        "\n",
        "    train_dataloader = load_data(train_data_path, tokenizer, batch_size=16, shuffle=True)\n",
        "    test_dataloader = load_data(test_data_path, tokenizer, batch_size=16, shuffle=False)\n",
        "\n",
        "    train_dataloaders.append(train_dataloader)\n",
        "    test_dataloaders.append(test_dataloader)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RKCkjD8lU3Ke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c5f0b057-d435-463f-db97-acc3997b876b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnum_blocks = 4\\n\\n# Lists to store train and test DataLoaders for each block\\ntrain_dataloaders = []\\ntest_dataloaders = []\\n\\n# Load data for each block\\nfor i in range(num_blocks):\\n    train_data_path = f'/content/drive/MyDrive/Text2Trade/data/block{i}_train.csv'\\n    test_data_path = f'/content/drive/MyDrive/Text2Trade/data/block{i}_test.csv'\\n\\n    train_dataloader = load_data(train_data_path, tokenizer, batch_size=16, shuffle=True)\\n    test_dataloader = load_data(test_data_path, tokenizer, batch_size=16, shuffle=False)\\n\\n    train_dataloaders.append(train_dataloader)\\n    test_dataloaders.append(test_dataloader)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "block0_train_dataloader = train_dataloaders[0]\n",
        "\n",
        "# Iterate through the dataloader to view the first batch\n",
        "for batch in block0_train_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    print(\"Input IDs:\", input_ids)\n",
        "    print(\"Attention Mask:\", attention_mask)\n",
        "    print(\"Labels:\", labels)\n",
        "    break  # Stop after viewing the first batch\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yp3GpUwIU4VV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f4763ac1-77c6-4df4-e223-b9fa2bf7fd45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nblock0_train_dataloader = train_dataloaders[0]\\n\\n# Iterate through the dataloader to view the first batch\\nfor batch in block0_train_dataloader:\\n    input_ids, attention_mask, labels = batch\\n    print(\"Input IDs:\", input_ids)\\n    print(\"Attention Mask:\", attention_mask)\\n    print(\"Labels:\", labels)\\n    break  # Stop after viewing the first batch\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "#we want to leave the last block as \"test\" after fine tuning.\n",
        "epochs = 4\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "training_stats = []\n",
        "block_validation_losses = []\n",
        "all_validation_losses = []\n",
        "\n",
        "for i in range(num_blocks):\n",
        "    # Split the data into training and validation for the current block\n",
        "    train_dataloader = train_dataloaders[i]\n",
        "    validation_dataloader = test_dataloaders[i]\n",
        "\n",
        "    #train your model on the current block's training data\n",
        "    for epoch_i in range(epochs):\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "\n",
        "        print(f'Epoch {epoch_i + 1}/{epochs}')\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "            b_input_ids = batch[0].squeeze(1)\n",
        "            b_input_mask = batch[1]\n",
        "            b_labels = batch[2]\n",
        "\n",
        "            #print(\"Shape of input_ids:\", b_input_ids.shape)\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(input_ids = b_input_ids,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "            #loss = outputs.loss\n",
        "            loss = nn.CrossEntropyLoss()(outputs.logits, b_labels)\n",
        "            logits = outputs.logits\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        model.eval()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "            b_input_ids = batch[0].squeeze(1)\n",
        "            b_input_mask = batch[1]\n",
        "            b_labels = batch[2]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids,\n",
        "                                attention_mask=b_input_mask)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(outputs.logits, b_labels)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            total_eval_accuracy += accuracy_score(np.argmax(logits, axis=1), label_ids)\n",
        "\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "        # Append the validation loss to the list for this block\n",
        "        block_validation_losses.append(avg_val_loss)\n",
        "\n",
        "    # Calculate the average validation loss for this block across all epochs\n",
        "    avg_validation_loss_block = sum(block_validation_losses) / len(block_validation_losses)\n",
        "    print(\"Average Validation Loss for Block {}: {:.2f}\".format(i, avg_validation_loss_block))\n",
        "\n",
        "    # Append the average validation loss for this block to the list for all blocks\n",
        "    all_validation_losses.append(avg_validation_loss_block)\n",
        "\n",
        "# Calculate the overall average validation loss across all blocks and epochs\n",
        "avg_validation_loss = sum(all_validation_losses) / len(all_validation_losses)\n",
        "print(\"Average Validation Loss Across Blocks and Epochs: {:.2f}\".format(avg_validation_loss))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t2iJPzhFVJNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "61ce316c-db23-4ef3-8b53-67c0c7e7f135"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\\n\\n#we want to leave the last block as \"test\" after fine tuning.\\nepochs = 4\\n# Total number of training steps is [number of batches] x [number of epochs].\\n# (Note that this is not the same as the number of training samples).\\ntotal_steps = len(train_dataloader) * epochs\\n\\n# Create the learning rate scheduler.\\nscheduler = get_linear_schedule_with_warmup(optimizer,\\n                                            num_warmup_steps = 0,\\n                                            num_training_steps = total_steps)\\n\\ntraining_stats = []\\nblock_validation_losses = []\\nall_validation_losses = []\\n\\nfor i in range(num_blocks):\\n    # Split the data into training and validation for the current block\\n    train_dataloader = train_dataloaders[i]\\n    validation_dataloader = test_dataloaders[i]\\n\\n    #train your model on the current block\\'s training data\\n    for epoch_i in range(epochs):\\n        # ========================================\\n        #               Training\\n        # ========================================\\n\\n        print(f\\'Epoch {epoch_i + 1}/{epochs}\\')\\n        print(\\'Training...\\')\\n\\n        total_train_loss = 0\\n        model.train()\\n\\n        for step, batch in enumerate(train_dataloader):\\n            if step % 40 == 0 and not step == 0:\\n                print(\\'  Batch {:>5,}  of  {:>5,}.\\'.format(step, len(train_dataloader)))\\n\\n            b_input_ids = batch[0].squeeze(1)\\n            b_input_mask = batch[1]\\n            b_labels = batch[2]\\n\\n            #print(\"Shape of input_ids:\", b_input_ids.shape)\\n\\n            model.zero_grad()\\n            outputs = model(input_ids = b_input_ids,\\n                            attention_mask=b_input_mask)\\n\\n            #loss = outputs.loss\\n            loss = nn.CrossEntropyLoss()(outputs.logits, b_labels)\\n            logits = outputs.logits\\n            total_train_loss += loss.item()\\n\\n            loss.backward()\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n            optimizer.step()\\n            scheduler.step()\\n\\n        avg_train_loss = total_train_loss / len(train_dataloader)\\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\\n\\n        # ========================================\\n        #               Validation\\n        # ========================================\\n        print(\"Running Validation...\")\\n\\n        model.eval()\\n        total_eval_accuracy = 0\\n        total_eval_loss = 0\\n        nb_eval_steps = 0\\n\\n        for batch in validation_dataloader:\\n            b_input_ids = batch[0].squeeze(1)\\n            b_input_mask = batch[1]\\n            b_labels = batch[2]\\n\\n            with torch.no_grad():\\n                outputs = model(b_input_ids,\\n                                attention_mask=b_input_mask)\\n\\n            loss = nn.CrossEntropyLoss()(outputs.logits, b_labels)\\n            logits = outputs.logits\\n\\n            total_eval_loss += loss.item()\\n            logits = logits.detach().cpu().numpy()\\n            label_ids = b_labels.to(\\'cpu\\').numpy()\\n\\n            total_eval_accuracy += accuracy_score(np.argmax(logits, axis=1), label_ids)\\n\\n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\\n        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\\n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\\n\\n        # Append the validation loss to the list for this block\\n        block_validation_losses.append(avg_val_loss)\\n\\n    # Calculate the average validation loss for this block across all epochs\\n    avg_validation_loss_block = sum(block_validation_losses) / len(block_validation_losses)\\n    print(\"Average Validation Loss for Block {}: {:.2f}\".format(i, avg_validation_loss_block))\\n\\n    # Append the average validation loss for this block to the list for all blocks\\n    all_validation_losses.append(avg_validation_loss_block)\\n\\n# Calculate the overall average validation loss across all blocks and epochs\\navg_validation_loss = sum(all_validation_losses) / len(all_validation_losses)\\nprint(\"Average Validation Loss Across Blocks and Epochs: {:.2f}\".format(avg_validation_loss))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}